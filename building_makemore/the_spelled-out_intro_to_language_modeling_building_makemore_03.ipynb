{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318e79a3-f93b-4011-bcaf-d638a6735f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building makemore Part 2: MLP(https://youtu.be/TCH_1BHY58I?si=uWMn6c-Wq1nPwAcX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e974b13-5222-4bb4-932c-eab4d494efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d2631d2-4967-4557-84ed-bb72cf3da8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().split()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92830573-b3c0-47bc-9f2a-63da8fbc96a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['.'] + [chr(i + ord('a')) for i in range(26)]\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {stoi[v]:v for v in stoi}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a0a66c39-5b7b-437a-a329-dd4d79fff341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crete_traininng_data(words, context_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        context = [stoi['.']]*context_length\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            xs.append(context)\n",
    "            ys.append(ix)\n",
    "            #print(f'{''.join([itos[i] for i in context])} -> {itos[ix]}')\n",
    "            context = context[1:] + [ix] # rolling window\n",
    "    return torch.tensor(xs), torch.tensor(ys)\n",
    "X,Y = crete_traininng_data(words,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10d99413-97ec-4ad1-a0ef-66440deef192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#considering it as a matrix manipulation\n",
    "#(one hot generates a tensor with the same number of rows as the input and the same number of colums as num classes\n",
    "#where for every row all the elements are 0 except for the one indexed by the input[row number] that equals 1)\n",
    "def embad_element(element, space, num_classes):\n",
    "    element_enc = F.one_hot(torch.tensor(element), num_classes).float()\n",
    "    #ex\n",
    "    #selector = F.one_hot(torch.tensor([1,2]), 3).float()\n",
    "    #selector\n",
    "    #tensor([[0., 1., 0.],\n",
    "    #        [0., 0., 1.]])\n",
    "    #space = [[a,b],[c,d],[e,f]]\n",
    "    #target = torch.tensor([[0,0,0],[1,1,1],[2,2,2]]).float()\n",
    "    #tensor([[0., 0., 0.],\n",
    "    #        [1., 1., 1.],\n",
    "    #        [2., 2., 2.]])\n",
    "    #result = selector @ target\n",
    "    #tensor([[1., 1., 1.],\n",
    "    #        [2., 2., 2.]])\n",
    "    \n",
    "    embaded_element = element_enc @ space\n",
    "    return embaded_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d12b65ee-2d6e-4bd6-9734-f6c0513cc290",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#embading space for name elements\n",
    "C = torch.randn(\n",
    "    (27,2) # dimension space of 27 by 2, we have 27 letters around 20k names\n",
    ")\n",
    "# training data embaded\n",
    "emb = C[X]\n",
    "\n",
    "# hiden layer\n",
    "W1 = torch.randn(\n",
    "    3*2, #3 embadings of 2 dimension 2\n",
    "    100, # for each neuron in the inputs for the hiden layers\n",
    ")\n",
    "b1 = torch.randn(100) # one for each neuron in the inputs for the hiden layers\n",
    "\n",
    "#unbinds with 1 turns emb(n,3,2) - list of n tensors of dim 6, \n",
    "#basically you specify to colaps the data on column(0), row(1) .... n-th dimension\n",
    "#that dimension will be droped and the result will be a set of row vectors created from all the elements in the input data in that dimension\n",
    "#cat takes a list or a list of lists of tensors and creates a list or a \n",
    "#list of lists of tensors where a level is colapsed and all the tensors at that level of the list turn in to one\n",
    "##reshaped_emb = torch.cat(torch.unbind(emb,1), 1) # convert our embading from a (n,3,2) in to an (n,6)\n",
    "#this is not very efficient we have views, pytorch keeps the data inside as single dimensional arrays(see .flatten() and untyped_storage()), views are free !\n",
    "# = emb.view(len(emb), 6)\n",
    "#emb.view(emb.shape[0], 6)\n",
    "#emb.view(2,-1, 6) #derive the other dimensions based on the other, only one can be infered, -1\n",
    "\n",
    "#trained embadings going trough the hiden layer\n",
    "h = torch.tanh(emb.view(emb.shape[0], 6) @ W1 + b1) # shape {emb.shape[0](aka Xs), 100 number of inputs for the hiden layer}\n",
    "\n",
    "W2 = torch.randn(\n",
    "    100, #number of inmputs for each neuron based on the hiden layer\n",
    "    27, #number of neurons and outputs\n",
    ")\n",
    "b2 = torch.randn(27) # one for each neuron in the output layer\n",
    "\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "#softmax\n",
    "# e^x/(e^x+e^y+e^z), e^y/(e^x+e^y+e^z), e^z(/e^x+e^y+e^z) for each row\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True) # sum per row keeping the results on the columns keepdim=True\n",
    "\n",
    "#relevant probes for our expected results\n",
    "expected_probs = prob[torch.arange(X.shape[0]),Y] \n",
    "# for every row aka input embading aka neuron in the output layer torch.arange(X.shape[0])\n",
    "#select the column with the activation for the output aka the probability that the network will produce the expected output\n",
    "\n",
    "#loss expected negative log mean probability\n",
    "loss = -expected_probs.log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "182aee36-a03d-42bd-aa05-93aef7e5ff7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.9120)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f9515e5e-8fdf-42d4-b72f-40be0e055a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size = 228146, training input size =32033\n",
      "num parameters = 3481\n",
      "start loss = 19.505229949951172\n",
      "final loss = 3.4696309566497803\n",
      "eatiy\n",
      "mialrerxny\n",
      "\n",
      "mi\n",
      "konn\n",
      "dkazieyeievhe\n",
      "aymii\n",
      "drienecaetepe\n",
      "doedwt\n",
      "rao\n"
     ]
    }
   ],
   "source": [
    "#remember if we represent a layer as a matrix we consider every row to be a neuron so we can apply them as x @ matrix so on the columns we have all the waits for a given input\n",
    "\n",
    "#cleand-up version\n",
    "def create_concrete_training_data(embading_size):\n",
    "    words = open('names.txt', 'r').read().split()\n",
    "    return crete_traininng_data(words, embading_size)\n",
    "\n",
    "def crate_model_parameters(alphabet_size, num_embading_dimensions, embading_size, number_of_neurons_in_the_hiden_layer, generator_seed):\n",
    "    #the input space is the same as the output space and the same as the alphabet\n",
    "    \n",
    "    #rng for consistency\n",
    "    g = torch.Generator().manual_seed(generator_seed)\n",
    "    #embading space\n",
    "    C = torch.randn((alphabet_size, num_embading_dimensions), generator=g, requires_grad=True)\n",
    "    #hiden layer\n",
    "    W1 = torch.randn((num_embading_dimensions * embading_size, number_of_neurons_in_the_hiden_layer), generator=g, requires_grad=True)\n",
    "    b1 = torch.randn(number_of_neurons_in_the_hiden_layer, generator=g, requires_grad=True)\n",
    "\n",
    "    #output layer\n",
    "    W2 = torch.randn((number_of_neurons_in_the_hiden_layer, alphabet_size), generator=g, requires_grad=True)\n",
    "    b2 = torch.randn(alphabet_size, generator=g, requires_grad=True)\n",
    "    return [g, C, W1, b1, W2, b2]\n",
    "\n",
    "def forward(parameters, xs, num_embading_dimensions, embading_size):\n",
    "    g, C, W1, b1, W2, b2 = parameters\n",
    "    #embade xs in C\n",
    "    emb = C[xs]\n",
    "    \n",
    "    #apply the hiden layer\n",
    "    h = torch.tanh(emb.view(-1,num_embading_dimensions * embading_size) @ W1 + b1)\n",
    "    \n",
    "    #apply the output layer\n",
    "    logits = h @ W2 + b2\n",
    "    \n",
    "    #soft max\n",
    "    counts = logits.exp()\n",
    "    prob = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "    return prob\n",
    "\n",
    "def calculate_loss(parameters, xs, ys, num_embading_dimensions, embading_size):\n",
    "    g, C, W1, b1, W2, b2 = parameters\n",
    "    #embade xs in C\n",
    "    emb = C[xs]\n",
    "    \n",
    "    #apply the hiden layer\n",
    "    h = torch.tanh(emb.view(-1,num_embading_dimensions * embading_size) @ W1 + b1)\n",
    "    \n",
    "    #apply the output layer\n",
    "    logits = h @ W2 + b2\n",
    "    \n",
    "    #soft max\n",
    "    #counts = logits.exp()\n",
    "    #prob = counts / counts.sum(1, keepdim=True)\n",
    "    #loss = -prob[torch.arange(len(ys)), ys].log().mean()\n",
    "    #this is the same thing\n",
    "    loss = F.cross_entropy(logits, ys)\n",
    "    return loss\n",
    "\n",
    "embeding_size = 3\n",
    "alphabet_size = 27\n",
    "num_embading_dimensions = 2\n",
    "number_of_hiden_nerons = 100\n",
    "generator_seed = 2147483647\n",
    "learning_ratre = 10\n",
    "\n",
    "#create training data\n",
    "xs,ys = create_concrete_training_data(embading_size)\n",
    "print(f'training data size = {xs.shape[0]}, training input size ={len(words)}')\n",
    "\n",
    "#create model\n",
    "parameters = crate_model_parameters(alphabet_size, num_embading_dimensions, embeding_size, number_of_hiden_nerons, generator_seed)\n",
    "num_parameters = sum(p.nelement() for p in parameters[1:])\n",
    "print(f'num parameters = {num_parameters}')\n",
    "\n",
    "#training\n",
    "rounds = 1000\n",
    "for current_round in range(rounds):\n",
    "    ##forward\n",
    "    loss = calculate_loss(parameters, xs, ys, num_embading_dimensions, embading_size)\n",
    "    \n",
    "    if current_round == 0:\n",
    "        print(f'start loss = {loss}')\n",
    "    elif (current_round + 1) == rounds:\n",
    "        print(f'final loss = {loss}')\n",
    "        \n",
    "    ##backward\n",
    "    for p in parameters[1:]:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    ##correction\n",
    "    for p in parameters[1:]:\n",
    "        p.data += -learning_ratre / (1.0 + current_round) * p.grad\n",
    "\n",
    "#use\n",
    "for _ in range(10):\n",
    "    ixs = [0] * embeding_size\n",
    "    output = []\n",
    "    while True:\n",
    "        prob = forward(parameters, torch.tensor([ixs]), num_embading_dimensions, embading_size)\n",
    "        ix = torch.multinomial(prob, 1, True, generator=parameters[0]).item()\n",
    "        if ix == 0:\n",
    "            break;\n",
    "        ixs = ixs[1:] + [ix]\n",
    "        #print(ixs)\n",
    "        output.append(itos[ix])\n",
    "    print(''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec157f8d-233b-4e19-a6fa-ca46e747596a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
